{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9-lQllbobDS",
        "outputId": "8e2ae229-5cc1-4e72-c60f-21884fb58c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVGjYhR_oyZ5",
        "outputId": "72caed1e-6491-4742-e189-2dccd09c4093"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/비컴최"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXWOBFbNoz0V",
        "outputId": "0cdaa588-8379-48f7-e47d-74f779989785"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/비컴최\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBPdWct7v0O9",
        "outputId": "c7d72d9a-b15f-41d2-9b1a-a5b62311d54a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import tensorflow.data as tfd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import Model, layers"
      ],
      "metadata": {
        "id": "HN-Vo4Lu2jRj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#하이퍼 파라미터 수정\n",
        "num_heads = 4\n",
        "embed_dim = 256\n",
        "ff_dim = 128\n",
        "vocab_size = 10000\n",
        "max_seq_len = 40\n",
        "\n",
        "learning_rate = 0.003\n",
        "epochs = 100\n",
        "batch_size = 32\n"
      ],
      "metadata": {
        "id": "gjqmsAEd2omI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up random seed for reproducibility\n",
        "random_seed = 123\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n"
      ],
      "metadata": {
        "id": "Cni5LLLV2rL8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = pd.read_csv(\"SPAM_text_message.csv\")\n",
        "\n",
        "data_frame.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1dKQWNxe2v3u",
        "outputId": "9a58ea67-c9e6-40b8-94c4-493d10d46d25"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Category                                            Message\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-723e4407-1131-4bcb-849a-10d44139fb0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-723e4407-1131-4bcb-849a-10d44139fb0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-723e4407-1131-4bcb-849a-10d44139fb0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-723e4407-1131-4bcb-849a-10d44139fb0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋에서 가장 긴 메시지의 길이를 계산하고, 최대 시퀀스 길이를 출력합니다.\n",
        "max_len = max([len(text) for text in data_frame.Message])\n",
        "print(f\"Maximum Length Of Input Sequence : {max_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLUMnqCp3HKt",
        "outputId": "7855f86a-8522-455c-df52-a43118e45f0b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Length Of Input Sequence : 910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 'Message' 열을 X로, 'Category' 열을 y로 추출합니다.\n",
        "X = data_frame['Message'].tolist()\n",
        "y = data_frame['Category'].tolist()\n",
        "\n",
        "\n",
        "# 레이블 인코더를 초기화하고, y값을 숫자로 변환합니다.\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f'X[:5]: \\n{X[:5]}\\n')\n",
        "print(f'y[:5]: {y[:5]}\\n')\n",
        "print(f\"Label Mapping : {label_encoder.inverse_transform(y[:5])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTVbJ9lS3MN-",
        "outputId": "58df8c7b-2479-4f5c-ed75-ad7265fd9468"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X[:5]: \n",
            "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\"]\n",
            "\n",
            "y[:5]: [0 0 1 0 0]\n",
            "\n",
            "Label Mapping : ['ham' 'ham' 'spam' 'ham' 'ham']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스의 불균형을 보정하기 위해 클래스 가중치를 계산합니다. 스팸과 비스팸 클래스 간의 비율을 고려하여 가중치를 계산하고 출력합니다.\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=data_frame.Category.unique(), y=label_encoder.inverse_transform(y))\n",
        "class_weights = {number: weight for number, weight in enumerate(class_weights)}\n",
        "\n",
        "print(f\"Associated class weights: {class_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mikoPdpV3xK-",
        "outputId": "e4103b65-8f89-428b-cdb7-708f167e710a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Associated class weights: {0: 0.5774093264248704, 1: 3.7295850066934406}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess the text\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    데이터 전처리 파트\n",
        "    \"\"\"\n",
        "    # 구두점을 공백으로 대체\n",
        "    text = tf.strings.regex_replace(text, f\"[{string.punctuation}]\", \" \")\n",
        "    \n",
        "    # 소문자로변환\n",
        "    text = tf.strings.lower(text)\n",
        "    \n",
        "    # 앞뒤 공백을 제거\n",
        "    text = tf.strings.strip(text)\n",
        "    \n",
        "    return text\n",
        "    \n",
        "\n",
        "# TextVectorization layer 생성합니다.\n",
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,                       # 최대 어휘 크기\n",
        "    output_sequence_length=max_seq_len,          # 최대 시퀀스 길이\n",
        "    standardize=preprocess_text,                 # 전처리 함수\n",
        "    pad_to_max_tokens=True,                      # Pad sequences to maximum length\n",
        "    output_mode='int'                            # integer-encoded sequences\n",
        ")\n",
        "\n",
        "# TextVectorization 레이어를 데이터에 적용하여 어휘를 생성하고, 어휘 사전을 구축합니다.\n",
        "text_vectorizer.adapt(X)"
      ],
      "metadata": {
        "id": "9SHrYM0130Wd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data set size\n",
        "N_SAMPLES = len(data_frame)\n",
        "\n",
        "print(f\"Total Number of Samples : {N_SAMPLES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhgEsBhC4CnQ",
        "outputId": "e2d2fcb7-4684-4726-ca73-1fe92c3e03e5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Samples : 5572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    # Send a text to randomly.\n",
        "    text_temp = X[np.random.randint(N_SAMPLES)]\n",
        "\n",
        "    # vectorization을 적용.\n",
        "    text_vec_temp = text_vectorizer(text_temp)\n",
        "\n",
        "    # results\n",
        "    print(f\"Original Text: {text_temp}\")\n",
        "    print(f\"Vectorized Text: {text_vec_temp}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXoKh35731-M",
        "outputId": "833e422b-823d-4e88-ee97-d6988f0f5cfc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: What was she looking for?\n",
            "Vectorized Text: [ 51  65  82 445  14   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0]\n",
            "\n",
            "Original Text: What does the dance river do?\n",
            "Vectorized Text: [  51  404    6 3946 3270   32    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "Original Text: K I'll be sure to get up before noon and see what's what\n",
            "Vectorized Text: [ 99   2  57  39 192   3  36  48 210 977   8  89  51  20  51   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0]\n",
            "\n",
            "Original Text: I don wake since. I checked that stuff and saw that its true no available spaces. Pls call the embassy or send a mail to them.\n",
            "Vectorized Text: [   2   92  484  470    2 1622   18  290    8  519   18   68  456   42\n",
            "  671 5190  109   17    6 7331   31   73    5  477    3  173    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "Original Text: Please da call me any mistake from my side sorry da. Pls da goto doctor.\n",
            "Vectorized Text: [ 107   95   17   11  111 1240   52   12  837   88   95  109   95 1261\n",
            " 1157    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘 사전을 가져와서 토큰을 확인합니다\n",
        "VOCAB = text_vectorizer.get_vocabulary()\n",
        "\n",
        "print(f\"Vocabulary size: {len(VOCAB)}\")\n",
        "print(f\"Vocabulary: {VOCAB[150:200]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP1YD2UN4Bm7",
        "outputId": "20e331ad-1fda-458f-f32c-d072c3fd3e25"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 8841\n",
            "Vocabulary: ['number', 'message', 'e', 've', 'tomorrow', 'say', 'won', 'right', 'prize', 'already', 'after', 'said', 'ask', 'doing', 'cash', 'amp', '3', 'yeah', 'really', 'im', 'why', 'b', 'life', 'them', 'meet', 'find', 'very', 'miss', 'morning', 'let', 'babe', 'last', 'would', 'win', 'thanks', 'cos', 'anything', 'uk', 'lol', 'also', 'care', 'every', 'sure', 'pick', 'com', '150p', 'sent', 'nokia', 'keep', 'urgent']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing으로 데이터를 분리함.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42, shuffle=True)\n",
        "\n",
        "# Apply the Text Vectorization\n",
        "X_train = text_vectorizer(X_train)\n",
        "X_test = text_vectorizer(X_test)\n",
        "\n",
        "# One Hot Vectors\n",
        "Xoh_train = tf.one_hot(X_train, depth=1000)\n",
        "Xoh_test  = tf.one_hot(X_test, depth=1000)"
      ],
      "metadata": {
        "id": "qf1cifV74J3R"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionalEmbedding(layers.Layer):\n",
        "    \n",
        "    def __init__(self, embedding_dims, vocab_size, seq_len, **kwargs):\n",
        "        super(TokenAndPositionalEmbedding, self).__init__(**kwargs)\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.seq_len = seq_len # 시퀀스의 길이\n",
        "        self.vocab_size = vocab_size # 어휘 사전의 크기\n",
        "        self.embedding_dims = embedding_dims # 임베딩 벡터의 차원\n",
        "        # 임베딩 벡터에 곱해지는 스케일 값을 계산하기 위함.\n",
        "        self.embed_scale = tf.math.sqrt(tf.cast(embedding_dims, tf.float32))\n",
        "        \n",
        "        # Define layers\n",
        "        # token 임베딩을 위한 임베딩 레이어, 어휘 사전의 크기와 임베딩 벡터의 차원을 입력으로 받음.\n",
        "        self.token_embedding = layers.Embedding(\n",
        "            input_dim=vocab_size, \n",
        "            output_dim=embedding_dims,\n",
        "            name=\"token_embedding\"\n",
        "        )\n",
        "        # 위칭 임베딩을 위한 임베딩 레이어, 시퀀스의 길이와 임베딩 벡터의 차원을 입력으로 받음 \n",
        "        self.positional_embedding = layers.Embedding(\n",
        "            input_dim=seq_len, \n",
        "            output_dim=embedding_dims,\n",
        "            name=\"positional_embedding\"\n",
        "        )\n",
        "    \n",
        "    def call(self, inputs): # 순방향 전파 연산을 정의함. 입력으로 들어온 시퀀스에 대해 토큰 임베딩과 위치 임베딩을 계산하여 결합한 결과를 반환함.\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        \n",
        "        # Token Embedding\n",
        "        token_embedding = self.token_embedding(inputs)\n",
        "        token_embedding *= self.embed_scale\n",
        "        \n",
        "        # Positional Embedding\n",
        "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "        positional_embedding = self.positional_embedding(positions)\n",
        "        \n",
        "        # Add Token and Positional Embedding\n",
        "        embeddings = token_embedding + positional_embedding\n",
        "        \n",
        "        return embeddings\n",
        "        \n",
        "    \n",
        "    def get_config(self): # 층의 설정을 반환함. 모델을 저장하거나 load 할 때, 층의 구성을 저장하고 복원하는데 사용.\n",
        "        config = super(TokenAndPositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'embedding_dims': self.embedding_dims,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'seq_len': self.seq_len,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "EByP0sMY4KdF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_embeds = TokenAndPositionalEmbedding(embed_dim, vocab_size, max_seq_len)(X_train[:1])\n",
        "temp_embeds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuEDC3vn4NeB",
        "outputId": "650b3d8a-4976-47ef-cb5c-e095121447e2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 40, 256), dtype=float32, numpy=\n",
              "array([[[-0.3880964 , -0.47288898, -0.24537694, ...,  0.40310794,\n",
              "         -0.44804114, -0.29323986],\n",
              "        [-0.40660465, -0.04319959, -0.03655719, ..., -0.52695304,\n",
              "         -0.13978596, -0.2532973 ],\n",
              "        [ 0.16900317,  0.70011663, -0.00110105, ...,  0.02513746,\n",
              "          0.4199832 , -0.13565637],\n",
              "        ...,\n",
              "        [ 0.63882357, -0.31927913,  0.103112  , ..., -0.3371863 ,\n",
              "         -0.11610547, -0.4128485 ],\n",
              "        [ 0.7090536 , -0.3397284 ,  0.15972938, ..., -0.31687164,\n",
              "         -0.1631437 , -0.37359652],\n",
              "        [ 0.6810206 , -0.27514577,  0.16277978, ..., -0.30336446,\n",
              "         -0.19594531, -0.3800351 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, num_heads: int, dropout_rate: float, embedding_dims: int, ff_dim: int, **kwargs):\n",
        "        super(TransformerLayer, self).__init__(**kwargs)\n",
        "        \n",
        "        # Initialize Parameters\n",
        "        self.num_heads = num_heads # layer head 개수\n",
        "        self.dropout_rate = dropout_rate # drop out 비율\n",
        "        self.embedding_dims = embedding_dims # 임베딩 차원\n",
        "        self.ff_dim = ff_dim # feedforward 네트워크 차원\n",
        "        \n",
        "        # Initialize Layers\n",
        "        # 레이어를 생성함. multi-head attetnion을 구현하는 부분. input을 세번 사용하여, query,key,value를 받음.\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dims, dropout=dropout_rate)\n",
        "        # 어텐션의 정규화 및 연결을 담당하는 layer\n",
        "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        # feedforward 내트워크를 구성하는 Sequential model. 두개의 연결 layer로 구성되어있으며, 활성화함수로 ReLU를 사용함.\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu', kernel_initializer='he_normal'),\n",
        "            layers.Dense(embedding_dims)\n",
        "        ])\n",
        "        # feedforward 네트워크 출력과 정규화를 담당하는 레이어.\n",
        "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    def call(self, inputs): # 순반향 전파 연산을 정의함.\n",
        "\n",
        "        # Multi-Head Attention\n",
        "        attention = self.mha(inputs, inputs, inputs)\n",
        "        \n",
        "        # Layer Normalization and Residual Connection\n",
        "        normalized1 = self.ln1(attention + inputs)\n",
        "        \n",
        "        # feedforward Network\n",
        "        ffn_out = self.ffn(normalized1)\n",
        "        \n",
        "        # Layer Normalization and Residual Connection\n",
        "        normalized2 = self.ln2(ffn_out + normalized1)\n",
        "        \n",
        "        return normalized2\n",
        "    \n",
        "    def get_config(self):\n",
        "\n",
        "        config = super(TransformerLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"embedding_dims\": self.embedding_dims,\n",
        "            \"ff_dim\": self.ff_dim\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "UxnvF_wt4Qpv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer layers execution\n",
        "TransformerLayer(num_heads=num_heads, embedding_dims=embed_dim, ff_dim=ff_dim, dropout_rate=0.1)(temp_embeds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrWqDBjP4R20",
        "outputId": "775d0eb3-4321-46ce-b5ff-bca7c2ea9d6d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 40, 256), dtype=float32, numpy=\n",
              "array([[[-0.5493694 , -0.23213398, -1.488012  , ..., -0.7851608 ,\n",
              "          0.5061727 ,  0.4329842 ],\n",
              "        [ 1.5696354 ,  1.0594529 , -2.4565094 , ...,  0.52035755,\n",
              "          0.7263887 , -0.67543054],\n",
              "        [-0.5230986 , -0.7380692 ,  0.04876898, ...,  1.7040778 ,\n",
              "         -0.4524423 , -0.8817183 ],\n",
              "        ...,\n",
              "        [ 1.8601035 ,  1.100482  , -1.7476187 , ...,  1.6482563 ,\n",
              "          0.15755595,  0.48714718],\n",
              "        [ 1.6561365 ,  0.9935242 , -1.8066171 , ...,  1.4416547 ,\n",
              "          0.1746852 ,  0.43709984],\n",
              "        [ 1.7067384 ,  1.0015274 , -1.8400493 , ...,  1.5454199 ,\n",
              "          0.12127464,  0.4311695 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input layer\n",
        "InputLayer = layers.Input(shape=(max_seq_len,), name=\"InputLayer\")\n",
        "\n",
        "# Embedding Layer\n",
        "embeddings = TokenAndPositionalEmbedding(embed_dim, vocab_size, max_seq_len, name=\"EmbeddingLayer\")(InputLayer)\n",
        "\n",
        "# Transformer Layer\n",
        "encodings = TransformerLayer(num_heads=num_heads, embedding_dims=embed_dim, ff_dim=ff_dim, dropout_rate=0.1, name=\"TransformerLayer\")(embeddings)\n",
        "\n",
        "# Classifier\n",
        "gap = layers.GlobalAveragePooling1D(name=\"GlobalAveragePooling\")(encodings)\n",
        "drop = layers.Dropout(0.5, name=\"Dropout\")(gap)\n",
        "OutputLayer = layers.Dense(1, activation='sigmoid', name=\"OutputLayer\")(drop)\n",
        "\n",
        "# Model\n",
        "model = keras.Model(InputLayer, OutputLayer, name=\"TransformerNet\")\n",
        "\n",
        "# Model Architecture Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMPLwR0O4TsY",
        "outputId": "1e61851f-b000-4e39-f459-765ccc88a73a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"TransformerNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " InputLayer (InputLayer)     [(None, 40)]              0         \n",
            "                                                                 \n",
            " EmbeddingLayer (TokenAndPos  (None, 40, 256)          2570240   \n",
            " itionalEmbedding)                                               \n",
            "                                                                 \n",
            " TransformerLayer (Transform  (None, 40, 256)          1118848   \n",
            " erLayer)                                                        \n",
            "                                                                 \n",
            " GlobalAveragePooling (Globa  (None, 256)              0         \n",
            " lAveragePooling1D)                                              \n",
            "                                                                 \n",
            " Dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " OutputLayer (Dense)         (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,689,345\n",
            "Trainable params: 3,689,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 컴파일\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=[\n",
        "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        keras.metrics.Precision(name='precision'),\n",
        "        keras.metrics.Recall(name='recall'),\n",
        "        keras.metrics.AUC(name='auc'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    class_weight=class_weights\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woJCVlzE4WYe",
        "outputId": "d6bf3c31-1339-4456-be3c-92db03eb828d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "112/112 [==============================] - 16s 105ms/step - loss: 2.4454e-08 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2478 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9590\n",
            "Epoch 2/100\n",
            "112/112 [==============================] - 7s 60ms/step - loss: 1.9923e-08 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2644 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9563\n",
            "Epoch 3/100\n",
            "112/112 [==============================] - 3s 29ms/step - loss: 7.2563e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2764 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9591\n",
            "Epoch 4/100\n",
            "112/112 [==============================] - 2s 16ms/step - loss: 4.5017e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2791 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 5/100\n",
            "112/112 [==============================] - 2s 20ms/step - loss: 3.2250e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2849 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 6/100\n",
            "112/112 [==============================] - 3s 26ms/step - loss: 3.5836e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2963 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 7/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 2.5463e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3049 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 8/100\n",
            "112/112 [==============================] - 2s 17ms/step - loss: 1.4711e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3023 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 9/100\n",
            "112/112 [==============================] - 2s 16ms/step - loss: 1.4091e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3041 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 10/100\n",
            "112/112 [==============================] - 2s 17ms/step - loss: 1.1680e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3057 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 11/100\n",
            "112/112 [==============================] - 2s 12ms/step - loss: 1.3399e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3083 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 12/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 9.6757e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3094 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 13/100\n",
            "112/112 [==============================] - 2s 16ms/step - loss: 1.3829e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3126 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 14/100\n",
            "112/112 [==============================] - 2s 16ms/step - loss: 7.3402e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3145 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 15/100\n",
            "112/112 [==============================] - 2s 18ms/step - loss: 8.6943e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3169 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 16/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 7.6173e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3183 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9558\n",
            "Epoch 17/100\n",
            "112/112 [==============================] - 2s 15ms/step - loss: 8.0883e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3243 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 18/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 6.1566e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3202 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 19/100\n",
            "112/112 [==============================] - 2s 15ms/step - loss: 5.5561e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3201 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 20/100\n",
            "112/112 [==============================] - 2s 15ms/step - loss: 5.5399e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3214 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 21/100\n",
            "112/112 [==============================] - 2s 17ms/step - loss: 5.1804e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 22/100\n",
            "112/112 [==============================] - 2s 15ms/step - loss: 5.4440e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3233 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9557\n",
            "Epoch 23/100\n",
            "112/112 [==============================] - 2s 15ms/step - loss: 4.6253e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3249 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 24/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.3831e-09 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3429 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9585\n",
            "Epoch 25/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 8.4695e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9585\n",
            "Epoch 26/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 6.1652e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9550\n",
            "Epoch 27/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 4.2216e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3476 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9550\n",
            "Epoch 28/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 3.9043e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3476 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 29/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 3.9704e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3476 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 30/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 3.3068e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 31/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 3.0740e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3459 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 32/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 2.7935e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3444 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 33/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 2.3628e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3430 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 34/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 2.5470e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 35/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 2.5003e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9558\n",
            "Epoch 36/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 3.1054e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3386 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9558\n",
            "Epoch 37/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 2.3255e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 38/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.9753e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3382 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 39/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 2.9398e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 40/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 2.4595e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3401 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 41/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 2.2264e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3407 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 42/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 2.2937e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 43/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 2.7668e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3420 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 44/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 5.3875e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9551\n",
            "Epoch 45/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.9290e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 46/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 2.1245e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 47/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.6901e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3501 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 48/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 1.4447e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3490 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 49/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.6637e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 50/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.6141e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3465 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 51/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.6937e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9558\n",
            "Epoch 52/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.8562e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3440 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 53/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.2415e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 54/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.7363e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 55/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.9933e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3438 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 56/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 1.6098e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3443 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 57/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 2.2383e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3449 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 58/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 2.1677e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3454 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 59/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.5244e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3459 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 60/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 3.8082e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3537 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 61/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 1.5487e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3528 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 62/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.5910e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 63/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.5054e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3498 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9558\n",
            "Epoch 64/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 1.3930e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 65/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 1.4487e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3472 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 66/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 4.8757e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3563 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 67/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.2271e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 68/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.6593e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3550 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 69/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.1259e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3542 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 70/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.2884e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9552\n",
            "Epoch 71/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.3170e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9558\n",
            "Epoch 72/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 1.4655e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 73/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.5195e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3532 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 74/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 1.0650e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3523 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 75/100\n",
            "112/112 [==============================] - 2s 16ms/step - loss: 1.1559e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3515 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 76/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.8409e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3515 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 77/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.2212e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3518 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 78/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 1.2844e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3525 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 79/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.4526e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3528 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 80/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.7376e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3525 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 81/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.3149e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3528 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 82/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.2570e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 83/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 1.7410e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3538 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 84/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 1.3654e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3542 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 85/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 2.0327e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3545 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 86/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 9.0711e-11 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3549 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 87/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.1387e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 88/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.7484e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3548 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 89/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.1101e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3551 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 90/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.4295e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3554 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 91/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 8.4499e-11 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3557 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9524\n",
            "Epoch 92/100\n",
            "112/112 [==============================] - 1s 13ms/step - loss: 1.5105e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3567 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 93/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 8.6806e-11 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3559 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 94/100\n",
            "112/112 [==============================] - 2s 14ms/step - loss: 1.0962e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.9765 - val_precision: 0.9343 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 95/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.2807e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3551 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 96/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.9189e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3558 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 97/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.3222e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3558 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 98/100\n",
            "112/112 [==============================] - 1s 12ms/step - loss: 1.0702e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3560 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 99/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 8.0915e-11 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3562 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n",
            "Epoch 100/100\n",
            "112/112 [==============================] - 1s 11ms/step - loss: 1.0561e-10 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3565 - val_accuracy: 0.9776 - val_precision: 0.9412 - val_recall: 0.9143 - val_auc: 0.9523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model performance on test data\n",
        "loss, acc, precision, recall, auc = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# model performance\n",
        "print('Test loss      :', loss)\n",
        "print('Test accuracy  :', acc*100)\n",
        "print('Test precision :', precision*100)\n",
        "print('Test recall    :', recall*100)\n",
        "print('Test AUC       :', auc*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_Yb3q314aYe",
        "outputId": "c30bd856-0103-4250-bae1-01e93a7446ab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss      : 0.20169119536876678\n",
            "Test accuracy  : 97.75784611701965\n",
            "Test precision : 93.05555820465088\n",
            "Test recall    : 89.9328887462616\n",
            "Test AUC       : 96.21215462684631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_tokens(tokens):\n",
        "\n",
        "    text = \" \".join(VOCAB[int(token)] for token in tokens).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "Lk3KaZLh4cg1"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    # Randomly select a text from the testing data.\n",
        "    index = np.random.randint(1,len(X_test))\n",
        "    tokens = X_test[index-1:index]\n",
        "    label = y_test[index]\n",
        "\n",
        "    # Feed the tokens to the model\n",
        "    print(f\"\\nModel Prediction\\n{'-'*100}\")\n",
        "    proba = 1 if model.predict(tokens, verbose=0)[0][0]>0.5 else 0\n",
        "    pred = label_encoder.inverse_transform([proba])\n",
        "    print(f\"Message: '{decode_tokens(tokens[0])}' | Prediction: {pred[0].title()} | True : {label_encoder.inverse_transform([label])[0].title()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZuhsHwI4fGA",
        "outputId": "f081be30-04dd-4b41-daca-d30e19e51d39"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'well thats nice too bad i cant eat it' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'height of oh shit situation a guy throws a luv letter on a gal but falls on her brothers head whos a gay d' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'on the way to office da' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'i had a good time too its nice to do something a bit different with my weekends for a change see ya soon' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'thank u it better work out cause i will feel used otherwise' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'don know this week i m going to tirunelvai da' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'never blame a day in ur life good days give u happiness bad days give u experience both are essential in life all are gods blessings good morning' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'don t make life too stressfull always find time to laugh it may not add years to your life but surely adds more life to ur years gud ni8 swt dreams' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'can you say what happen' | Prediction: Ham | True : Ham\n",
            "\n",
            "\n",
            "Model Prediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'the sign of maturity is not when we start saying big things but actually it is when we start understanding small things have a nice evening bslvyl' | Prediction: Ham | True : Ham\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Input\n",
        "text = input(\"Enter your Msg: \")\n",
        "\n",
        "# Convert into tokens\n",
        "tokens = text_vectorizer([text])\n",
        "\n",
        "# Feed the tokens to the model\n",
        "print(f\"\\nModel Predictions\\n{'-'*100}\")\n",
        "proba = 1 if model.predict(tokens, verbose=0)[0][0]>0.5 else 0\n",
        "pred = label_encoder.inverse_transform([proba])\n",
        "print(f\"Message: '{text}' | Prediction: {pred[0].title()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tEyBaDJ4gw-",
        "outputId": "c976f185-04bc-4d44-e7cc-a4333a37bc2c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Msg: This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.\n",
            "\n",
            "Model Predictions\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Message: 'This is the 2nd time we have tried 2 contact u. U have won the £750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.' | Prediction: Spam\n"
          ]
        }
      ]
    }
  ]
}